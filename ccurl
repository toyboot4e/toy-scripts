#!/usr/bin/env bash
#
# Cached `curl`, useful for temporary scraping.
# Be sure that `ccurl` requires the first argument to be an URL!

cache_dir="$HOME/.cache/ccurl"

if [ ! -d "$cache_dir" ] ; then
    mkdir -p "$cache_dir"
fi

_run() {
    if [ $# -eq 0 ] ; then
        echo "given no URL"
        return
    fi

    case "${1:-}" in
        'c' | 'clear')
            rm -rf ~/.cache/ccurl/*  > /dev/null 2>&1
            rm -rf ~/.cache/ccurl/.* > /dev/null 2>&1
            return
            ;;
    esac

    # we assume the first argument is URL
    url="$1"

    # replace `/` in the URL with `@`
    file_name="$(printf '%s' "$url" | sed 's;/;@;g')"
    file="$cache_dir/$file_name"

    if [ -f "$file" ] ; then
        # cache found
        cat "$file"
        return
    fi

    # call curl and create cache
    curl "$@" | tee "$file"

    # remove cache on failure
    if [ $? != 0 ] ; then
        rm "$file"
    fi
}

_run "$@"
